---
title: 信息论
tag: 信息论
category: 课内
---



呃呃

> 这玩意就是信息论课的内容了 以后更估计也就在这篇里面更了 大概

<!-- more -->

---

以下内容更新于2021-10-31

---

# 信息度量

## 离散信息度量

> 首先肯定是经典的各种符号约定,顺遍让我复习一哈LateX语法...

- 随机事件：$x$
- 随机变量：$X$
- $x=a_i的概率$：$P_x(a_i)$
- 联合概率：$P_{xy}(a_i,b_j)$



### 自信息

事件集合X中$x=a_i$的自信息

> $I_x(a_i)=-logP_x(a_i)$
>
> $I(x)=-logp(x)$



**底数可变**

- $log_2x$：比特
- $lnx$：奈特
- $log_{10}x$：哈特



**自信息的含义即**

>事件发生前的不确定性

>事件发生后事件包含的信息量



**联合自信息**

即一些事件看作一个联合事件后计算自信息，概率为联合概率

> $I_{XY}(a_i,b_j)=-logP_{XY}(a_i,b_j)$
>
> $I(xy)=-logp(xy)$
>
> $xy$视为一联合事件



**条件自信息**

即给定一些事件后，发生其他事件的自信息

> $I_{x|y}(a_i,b_j)=-logP_{X|Y}(a_i,b_j)$
>
> $I(x|y)=-logp(x|y)$
>
> 因此有：
>
> $I(xy)=I(x)+I(y|x)=I(y)+I(x|y)$



**综上**

- 自信息量表示一个/多个事件发生带给我们信息量的大小
- 表示了确定一个/多个事件是否发生，所需要的信息量的大小
- $log_2$情况下表示了表示出事件信息量所需要的二进制位的个数

---

### 互信息

- 反映了两个事件之间的统计关联程度
- 通信系统中，意义是输出端接受到某信息y后获得关于输入端某信息x的信息量

> $I_{x;y}(a_i;b_j)=log\frac{P_{X/Y}(a_i|b_j)}{P_X(a_i)}$
>
> $I(x;y)=I(x)-I(x|y)$	//$x$本身的不确定性减去由$y$确定的$x$的不确定性
>
> $I(x;y)=I(y;x)$

---



### 信息熵

**离散信源X的熵定义位自信息的平均值，记为H(X)**

> $H(X)=E[I(x)]$
>
> 单位：比特/符号

表现一个信源的平均不确定性/平均信息量、$H(x)$大的随机性大、输出后解除信息不确定行需要的信息量。



**条件熵**

**联合集$XY$上，条件自信息$I(y|x)$均值**

> $H(Y|X)=E[I(y|x)]=\sum_{x}{p(x)H(Y|X=x)}$



**联合熵**

**联合集$XY$上，条件自信息$I(xy)$均值**

> $H(XY)=E[U(xy)]=-\sum_x\sum_yp(xy)logp(xy)$



**相对熵——信息散度**

> 没懂。

> 不等式：$1-\frac{1}{x}\leq lnx \leq x-1$

**熵的不增原理——条件熵不大于信息熵**

> $H(Y|X)\leq H(Y)$

---

### 信息熵基本性质

- 对称性
- 非负性
- 确定性：集合中任意事件概率为1时，熵为0
- 扩展性：小概率事件对熵影响很小可以忽略
- 可加性：$H(XY)=H(X)+H(Y|X)$

> 熵的链原则：$H(X_1X_2..X_N)=H(X_1)+H(X_2|X_1)+...H(X_N|X_1..X_{N-1})$

- 极值性：集合中事件等概率发生时熵达到最大

---

### 平均互信息

> $I(X;Y)=\sum_xp(x)I(x;Y)$
>
> $=\sum_{x,y}p(x)p(y/x)log\frac{p(y/x)}{p(y)}$

