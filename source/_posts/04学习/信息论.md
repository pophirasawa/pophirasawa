---
title: 信息论
tag: 信息论
category: 课内
---



呃呃

> 这玩意就是信息论课的内容了 以后更估计也就在这篇里面更了 大概

<!-- more -->

---

以下内容更新于2021-10-31

---

# 信息度量

## 离散信息度量

> 首先肯定是经典的各种符号约定,顺遍让我复习一哈LateX语法...

- 随机事件：$x$

- 随机变量：$X$

- $x=a_i的概率$：$P_x(a_i)$

- 联合概率：$P_{xy}(a_i,b_j)$

### 自信息

事件集合X中$x=a_i$的自信息

> $I_x(a_i)=-logP_x(a_i)$
>
> $I(x)=-logp(x)$

**底数可变**

- $log_2x$：比特
- $lnx$：奈特
- $log_{10}x$：哈特

**自信息的含义即**

>事件发生前的不确定性

>事件发生后事件包含的信息量

**联合自信息**

即一些事件看作一个联合事件后计算自信息，概率为联合概率

> $I_{XY}(a_i,b_j)=-logP_{XY}(a_i,b_j)$
>
> $I(xy)=-logp(xy)$
>
> $xy$视为一联合事件

**条件自信息**

即给定一些事件后，发生其他事件的自信息

> $I_{x|y}(a_i,b_j)=-logP_{X|Y}(a_i,b_j)$
>
> $I(x|y)=-logp(x|y)$
>
> 因此有：
>
> $I(xy)=I(x)+I(y|x)=I(y)+I(x|y)$

**综上**

- 自信息量表示一个/多个事件发生带给我们信息量的大小
- 表示了确定一个/多个事件是否发生，所需要的信息量的大小
- $log_2$情况下表示了表示出事件信息量所需要的二进制位的个数



### 互信息

- 反映了两个事件之间的统计关联程度
- 通信系统中，意义是输出端接受到某信息y后获得关于输入端某信息x的信息量
